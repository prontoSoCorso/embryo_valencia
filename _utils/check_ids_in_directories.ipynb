{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fdf87be",
   "metadata": {},
   "source": [
    "## Check if the extracted pdbs have the same ids of the csv final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d301a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directory: /home/phd2/Documenti/embryo/valencia/extracted_equatorial_frames ...\n",
      "Total folders found in filesystem: 211\n",
      "\n",
      "========================================\n",
      "MATCHING REPORT\n",
      "========================================\n",
      "Total IDs in CSV:       204\n",
      "IDs found in Folders:   159  (77.9%)\n",
      "IDs MISSING images:     45  (22.1%)\n",
      "========================================\n",
      "\n",
      "Breakdown by Group (Note):\n",
      "has_images        False  True \n",
      "Note                          \n",
      "GV_to_GV              7     29\n",
      "GV_to_M1              4      8\n",
      "GV_to_M2             11     36\n",
      "M2_to_blasto         10     30\n",
      "M2_to_no_blasto       4     16\n",
      "rM2_to_blasto         2      6\n",
      "rM2_to_no_blasto      7     34\n",
      "\n",
      "--- Example Missing IDs (First 5) ---\n",
      "     D2016.11.14_S1895_I106_6\n",
      "     D2016.11.14_S1895_I106_7\n",
      " D2018.12.21_S02510_I0106_D_1\n",
      " D2018.12.21_S02510_I0106_D_2\n",
      "D2019.02.15_S012161_I0631_D_1\n",
      "\n",
      "--- Example Found IDs (First 5) ---\n",
      "    D2017.09.15_S0770_I631_2\n",
      "    D2017.09.15_S0770_I631_3\n",
      "    D2017.09.15_S0770_I631_4\n",
      "D2018.09.18_S01710_I0406_D_1\n",
      "D2018.09.18_S01710_I0406_D_2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "CSV_PATH = \"/home/phd2/Scrivania/CorsoRepo/embryo_valencia/dataset_final_merged.csv\"\n",
    "ROOT_IMG_DIR = \"/home/phd2/Documenti/embryo/valencia/extracted_equatorial_frames\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. SCAN DIRECTORIES\n",
    "# ==========================================\n",
    "print(f\"Scanning directory: {ROOT_IMG_DIR} ...\")\n",
    "\n",
    "# Set to store names of all folders found in the directory tree\n",
    "found_folders = set()\n",
    "folder_paths = {} # Dictionary to store ID -> Full Path mapping (optional but useful)\n",
    "\n",
    "# os.walk yields a 3-tuple (dirpath, dirnames, filenames)\n",
    "for root, dirs, files in os.walk(ROOT_IMG_DIR):\n",
    "    for dirname in dirs:\n",
    "        # We assume the directory name is the ID (e.g., D2017.09.15_S0770_I631_1)\n",
    "        clean_dirname = dirname.strip()\n",
    "        found_folders.add(clean_dirname)\n",
    "        folder_paths[clean_dirname] = os.path.join(root, dirname)\n",
    "\n",
    "print(f\"Total folders found in filesystem: {len(found_folders)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. LOAD DATASET & MATCH\n",
    "# ==========================================\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "    # Create a new column to indicate availability\n",
    "    # We use boolean indexing to check membership in the set\n",
    "    df['has_images'] = df['dish_well'].apply(lambda x: str(x).strip() in found_folders)\n",
    "    \n",
    "    # Optional: Store the path if found\n",
    "    df['image_path'] = df['dish_well'].apply(lambda x: folder_paths.get(str(x).strip(), None))\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. REPORT STATISTICS\n",
    "    # ==========================================\n",
    "    total_ids = len(df)\n",
    "    found_count = df['has_images'].sum()\n",
    "    missing_count = total_ids - found_count\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"MATCHING REPORT\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Total IDs in CSV:       {total_ids}\")\n",
    "    print(f\"IDs found in Folders:   {found_count}  ({(found_count/total_ids)*100:.1f}%)\")\n",
    "    print(f\"IDs MISSING images:     {missing_count}  ({(missing_count/total_ids)*100:.1f}%)\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Show breakdown by Note (Outcome/Group)\n",
    "    print(\"\\nBreakdown by Group (Note):\")\n",
    "    print(df.groupby('Note')['has_images'].value_counts().unstack().fillna(0))\n",
    "    \n",
    "    # Show missing examples (first 5)\n",
    "    if missing_count > 0:\n",
    "        print(\"\\n--- Example Missing IDs (First 5) ---\")\n",
    "        print(df[~df['has_images']]['dish_well'].head(5).to_string(index=False))\n",
    "        \n",
    "    # Show found examples (first 5)\n",
    "    if found_count > 0:\n",
    "        print(\"\\n--- Example Found IDs (First 5) ---\")\n",
    "        print(df[df['has_images']]['dish_well'].head(5).to_string(index=False))\n",
    "\n",
    "    # Optional: Save the report/updated CSV\n",
    "    # df.to_csv(CSV_PATH.replace(\".csv\", \"_checked.csv\"), index=False)\n",
    "    \n",
    "else:\n",
    "    print(f\"Error: CSV file not found at {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406c08e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSTIC MODE ---\n",
      "\n",
      "Total Folders on Disk: 211\n",
      "Total IDs in CSV:      204\n",
      "Missing IDs:           45\n",
      "\n",
      "==================================================\n",
      "MISMATCH ANALYSIS\n",
      "==================================================\n",
      "analyzing first missing ID: 'D2016.11.14_S1895_I106_6'\n",
      "(Length: 24 chars)\n",
      "\n",
      "Searching disk for folders containing 'S1895'...\n",
      "No folders found with that Sample ID. The folder might use a completely different naming convention.\n",
      "\n",
      "==================================================\n",
      "RAW STRING DUMP (Check for spaces)\n",
      "==================================================\n",
      "First 3 Missing IDs from CSV:\n",
      "  'D2016.11.14_S1895_I106_6'\n",
      "  'D2016.11.14_S1895_I106_7'\n",
      "  'D2018.12.21_S02510_I0106_D_1'\n",
      "\n",
      "First 3 Folders found on Disk:\n",
      "  'D2021.09.21_S01044_I3026_P_2'\n",
      "  'D2020.06.01_S01605_I0631_D_5'\n",
      "  '2022'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP\n",
    "# ==========================================\n",
    "CSV_PATH = \"/home/phd2/Scrivania/CorsoRepo/embryo_valencia/dataset_final_merged.csv\"\n",
    "ROOT_IMG_DIR = \"/home/phd2/Documenti/embryo/valencia/extracted_equatorial_frames\"\n",
    "\n",
    "print(\"--- DIAGNOSTIC MODE ---\")\n",
    "\n",
    "# 1. Load Folder Names (Raw)\n",
    "disk_folders = set()\n",
    "for root, dirs, files in os.walk(ROOT_IMG_DIR):\n",
    "    for d in dirs:\n",
    "        disk_folders.add(d)\n",
    "\n",
    "# 2. Load CSV IDs\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    csv_ids = df['dish_well'].dropna().astype(str).tolist()\n",
    "    \n",
    "    # 3. Find Missing\n",
    "    # We check for EXACT match first\n",
    "    missing_ids = [i for i in csv_ids if i not in disk_folders]\n",
    "    \n",
    "    print(f\"\\nTotal Folders on Disk: {len(disk_folders)}\")\n",
    "    print(f\"Total IDs in CSV:      {len(csv_ids)}\")\n",
    "    print(f\"Missing IDs:           {len(missing_ids)}\")\n",
    "    \n",
    "    if len(missing_ids) > 0:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MISMATCH ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Take the first missing ID to analyze\n",
    "        target_id = missing_ids[0]\n",
    "        print(f\"analyzing first missing ID: '{target_id}'\")\n",
    "        print(f\"(Length: {len(target_id)} chars)\")\n",
    "        \n",
    "        # 4. Search for 'Close Suspects' on Disk\n",
    "        # We try to find this ID inside the folder names, or vice versa, or by sample code\n",
    "        # Extract a core chunk (e.g., the Sample ID part 'S1895')\n",
    "        import re\n",
    "        # Try to find 'S' followed by 4 digits\n",
    "        match = re.search(r'(S\\d{4,5})', target_id)\n",
    "        if match:\n",
    "            core_pattern = match.group(1)\n",
    "            print(f\"\\nSearching disk for folders containing '{core_pattern}'...\")\n",
    "            suspects = [f for f in disk_folders if core_pattern in f]\n",
    "            \n",
    "            if suspects:\n",
    "                print(f\"Found {len(suspects)} potential matches on disk. Comparing:\")\n",
    "                print(f\"{'CSV EXPECTS':<40} | {'ACTUAL FOLDER ON DISK'}\")\n",
    "                print(\"-\" * 80)\n",
    "                for s in suspects[:5]: # Show top 5\n",
    "                    print(f\"'{target_id}'\\n   vs\\n'{s}'\")\n",
    "                    print(\"-\" * 30)\n",
    "                    \n",
    "                    # Check for common invisible issues\n",
    "                    if target_id.strip() == s.strip() and target_id != s:\n",
    "                        print(\"   -> DETECTED: Whitespace mismatch (check spaces at start/end)\")\n",
    "                    elif target_id.lower() == s.lower() and target_id != s:\n",
    "                        print(\"   -> DETECTED: Case mismatch (Upper/Lower case)\")\n",
    "                    elif target_id.replace('.', '-') == s or target_id == s.replace('.', '-'):\n",
    "                        print(\"   -> DETECTED: Date separator mismatch (. vs -)\")\n",
    "            else:\n",
    "                print(\"No folders found with that Sample ID. The folder might use a completely different naming convention.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RAW STRING DUMP (Check for spaces)\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"First 3 Missing IDs from CSV:\")\n",
    "        for i in missing_ids[:3]:\n",
    "            print(f\"  '{i}'\")\n",
    "            \n",
    "        print(\"\\nFirst 3 Folders found on Disk:\")\n",
    "        for i in list(disk_folders)[:3]:\n",
    "            print(f\"  '{i}'\")\n",
    "            \n",
    "else:\n",
    "    print(\"CSV not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db64359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DEEP DEBUGGING: D2018.12.21_S02510_I0106_D_1 ---\n",
      "\n",
      "1. Checking Filesystem Path: /home/phd2/Documenti/embryo/valencia/extracted_equatorial_frames/2018/D2018.12.21_S02510_I0106_D_1\n",
      "   [FAIL] Path does NOT exist. Check typos in parent path.\n",
      "\n",
      "2. Checking CSV Entry in: /home/phd2/Scrivania/CorsoRepo/embryo_valencia/dataset_final_merged.csv\n",
      "   [OK] Found row in CSV. ID: 'D2017.09.15_S0770_I631_3'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# CONFIG\n",
    "# ==========================================\n",
    "CSV_PATH = \"/home/phd2/Scrivania/CorsoRepo/embryo_valencia/dataset_final_merged.csv\"\n",
    "TARGET_DIR_PARENT = \"/home/phd2/Documenti/embryo/valencia/extracted_equatorial_frames/2018\"\n",
    "TARGET_FOLDER_NAME = \"D2018.12.21_S02510_I0106_D_1\"\n",
    "\n",
    "print(\"--- DEEP DEBUGGING: D2018.12.21_S02510_I0106_D_1 ---\")\n",
    "\n",
    "# 1. CHECK FILESYSTEM VISIBILITY\n",
    "full_target_path = os.path.join(TARGET_DIR_PARENT, TARGET_FOLDER_NAME)\n",
    "print(f\"\\n1. Checking Filesystem Path: {full_target_path}\")\n",
    "if os.path.exists(full_target_path):\n",
    "    print(\"   [OK] Path exists on disk.\")\n",
    "    \n",
    "    # Get the actual string from the OS to compare bytes\n",
    "    try:\n",
    "        actual_folders = os.listdir(TARGET_DIR_PARENT)\n",
    "        # Find the one that looks like our target\n",
    "        fs_name = next(f for f in actual_folders if TARGET_FOLDER_NAME in f)\n",
    "        print(f\"   [INFO] Name read from disk: '{fs_name}'\")\n",
    "    except StopIteration:\n",
    "        print(\"   [ERROR] Path exists but os.listdir() couldn't find the entry. Permission issue?\")\n",
    "        fs_name = \"ERROR\"\n",
    "else:\n",
    "    print(\"   [FAIL] Path does NOT exist. Check typos in parent path.\")\n",
    "    fs_name = \"ERROR\"\n",
    "\n",
    "# 2. CHECK CSV DATA\n",
    "print(f\"\\n2. Checking CSV Entry in: {CSV_PATH}\")\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    # Filter for the target\n",
    "    # We use 'str.contains' to be loose first, just to find the row\n",
    "    subset = df[df['dish_well'].astype(str).str.contains(\"S0770_I631_3\", na=False)]\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        csv_name = subset.iloc[0]['dish_well']\n",
    "        print(f\"   [OK] Found row in CSV. ID: '{csv_name}'\")\n",
    "    else:\n",
    "        print(\"   [FAIL] Could not find this ID in the CSV at all.\")\n",
    "        csv_name = \"ERROR\"\n",
    "else:\n",
    "    print(\"   [FAIL] CSV file missing.\")\n",
    "    csv_name = \"ERROR\"\n",
    "\n",
    "# 3. BYTE-LEVEL COMPARISON\n",
    "if fs_name != \"ERROR\" and csv_name != \"ERROR\":\n",
    "    print(\"\\n3. BYTE-LEVEL COMPARISON\")\n",
    "    print(\"   We will print the ASCII/Unicode value of every character.\")\n",
    "    print(\"   If they differ, you will see it here.\")\n",
    "    \n",
    "    print(f\"\\n   {'Index':<5} | {'CSV Char':<10} {'Code':<10} || {'DISK Char':<10} {'Code':<10} | {'Match?'}\")\n",
    "    print(\"   \" + \"-\"*70)\n",
    "    \n",
    "    max_len = max(len(csv_name), len(fs_name))\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        # CSV Char\n",
    "        if i < len(csv_name):\n",
    "            c_char = csv_name[i]\n",
    "            c_code = ord(c_char)\n",
    "            c_disp = repr(c_char)\n",
    "        else:\n",
    "            c_char, c_code, c_disp = \"END\", \"\", \"\"\n",
    "            \n",
    "        # Disk Char\n",
    "        if i < len(fs_name):\n",
    "            d_char = fs_name[i]\n",
    "            d_code = ord(d_char)\n",
    "            d_disp = repr(d_char)\n",
    "        else:\n",
    "            d_char, d_code, d_disp = \"END\", \"\", \"\"\n",
    "            \n",
    "        match = \"OK\" if c_char == d_char else \"MISMATCH <---\"\n",
    "        print(f\"   {i:<5} | {c_disp:<10} {c_code:<10} || {d_disp:<10} {d_code:<10} | {match}\")\n",
    "\n",
    "    if csv_name == fs_name:\n",
    "        print(\"\\n   [RESULT] Strings are IDENTICAL. If logic failed, it was the search algorithm, not the strings.\")\n",
    "    else:\n",
    "        print(\"\\n   [RESULT] Strings are DIFFERENT. See the mismatch arrow above.\")\n",
    "        print(\"   Common causes: 160 (Non-breaking space) vs 32 (Space), or hidden BOM markers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901bb690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directory: /home/phd2/Documenti/embryo/valencia/extracted_equatorial_frames\n",
      "Folders found on disk: 211\n",
      "\n",
      "========================================\n",
      "VERIFYING TARGET: D2018.12.21_S02510_I0106_D_1\n",
      "========================================\n",
      "[DISK] NOT found in df_files. (This would be very strange given the previous debug)\n",
      "[MERGE] Final Status: MISSING\n",
      "[MERGE] Path mapped: nan\n",
      "\n",
      "========================================\n",
      "FINAL ACCURATE REPORT\n",
      "========================================\n",
      "Total IDs:      204\n",
      "Found:          159 (77.9%)\n",
      "Missing:        45\n",
      "\n",
      "--- Genuine Missing IDs (First 5) ---\n",
      "     D2016.11.14_S1895_I106_6\n",
      "     D2016.11.14_S1895_I106_7\n",
      " D2018.12.21_S02510_I0106_D_1\n",
      " D2018.12.21_S02510_I0106_D_2\n",
      "D2019.02.15_S012161_I0631_D_1\n",
      "\n",
      "Saved updated dataset to: /home/phd2/Scrivania/CorsoRepo/embryo_valencia/dataset_final_merged_with_paths.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "CSV_PATH = \"/home/phd2/Scrivania/CorsoRepo/embryo_valencia/dataset_final_merged.csv\"\n",
    "ROOT_IMG_DIR = \"/home/phd2/Documenti/embryo/valencia/extracted_equatorial_frames\"\n",
    "\n",
    "print(f\"Scanning directory: {ROOT_IMG_DIR}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. ROBUST FILESYSTEM SCAN\n",
    "# ==========================================\n",
    "# We collect specific details to debug matches\n",
    "found_data = []\n",
    "\n",
    "for root, dirs, files in os.walk(ROOT_IMG_DIR):\n",
    "    for dirname in dirs:\n",
    "        # We store the cleaned name AND the full path\n",
    "        clean_name = dirname.strip()\n",
    "        full_path = os.path.join(root, dirname)\n",
    "        found_data.append({\n",
    "            'filesystem_id': clean_name,\n",
    "            'image_path': full_path\n",
    "        })\n",
    "\n",
    "# Create a DataFrame of what is actually on the disk\n",
    "df_files = pd.DataFrame(found_data)\n",
    "print(f\"Folders found on disk: {len(df_files)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. LOAD & MERGE\n",
    "# ==========================================\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df_data = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "    # Ensure ID column is string and stripped of whitespace\n",
    "    df_data['dish_well'] = df_data['dish_well'].astype(str).str.strip()\n",
    "    \n",
    "    # Check for duplicates in CSV (which might skew counts)\n",
    "    if df_data['dish_well'].duplicated().any():\n",
    "        print(f\"Note: {df_data['dish_well'].duplicated().sum()} duplicate IDs found in CSV.\")\n",
    "\n",
    "    # --- THE MERGE ---\n",
    "    # Left join: Keep all CSV rows, add path if found\n",
    "    df_merged = pd.merge(\n",
    "        df_data, \n",
    "        df_files, \n",
    "        left_on='dish_well', \n",
    "        right_on='filesystem_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Define 'has_images' based on whether the path is NaN or not\n",
    "    df_merged['has_images'] = df_merged['image_path'].notna()\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. VERIFICATION OF TARGET ID\n",
    "    # ==========================================\n",
    "    target_id = \"D2018.12.21_S02510_I0106_D_1\"\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"VERIFYING TARGET: {target_id}\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Check if this ID exists in the FILES dataframe\n",
    "    file_check = df_files[df_files['filesystem_id'] == target_id]\n",
    "    if not file_check.empty:\n",
    "        print(f\"[DISK] Found in df_files! Path: {file_check.iloc[0]['image_path']}\")\n",
    "    else:\n",
    "        print(f\"[DISK] NOT found in df_files. (This would be very strange given the previous debug)\")\n",
    "\n",
    "    # Check the merge result\n",
    "    row = df_merged[df_merged['dish_well'] == target_id]\n",
    "    if not row.empty:\n",
    "        status = row.iloc[0]['has_images']\n",
    "        path = row.iloc[0]['image_path']\n",
    "        print(f\"[MERGE] Final Status: {'FOUND' if status else 'MISSING'}\")\n",
    "        print(f\"[MERGE] Path mapped: {path}\")\n",
    "    else:\n",
    "        print(\"[MERGE] ID not found in dataset dataframe.\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. FINAL REPORT\n",
    "    # ==========================================\n",
    "    found_count = df_merged['has_images'].sum()\n",
    "    missing_count = len(df_merged) - found_count\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"FINAL ACCURATE REPORT\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Total IDs:      {len(df_merged)}\")\n",
    "    print(f\"Found:          {found_count} ({(found_count/len(df_merged))*100:.1f}%)\")\n",
    "    print(f\"Missing:        {missing_count}\")\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        print(\"\\n--- Genuine Missing IDs (First 5) ---\")\n",
    "        print(df_merged[~df_merged['has_images']]['dish_well'].head(5).to_string(index=False))\n",
    "\n",
    "    # Save\n",
    "    save_path = CSV_PATH.replace(\".csv\", \"_with_paths.csv\")\n",
    "    \n",
    "    # Clean up output (remove the helper column 'filesystem_id')\n",
    "    df_final = df_merged.drop(columns=['filesystem_id'])\n",
    "    df_final.to_csv(save_path, index=False)\n",
    "    print(f\"\\nSaved updated dataset to: {save_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"CSV not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b7131b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- COMPLETE LIST OF MISSING IDs ---\n",
      "  'D2016.11.14_S1895_I106_6'\n",
      "  'D2016.11.14_S1895_I106_7'\n",
      "  'D2018.12.21_S02510_I0106_D_1'\n",
      "  'D2018.12.21_S02510_I0106_D_2'\n",
      "  'D2019.02.15_S012161_I0631_D_1'\n",
      "  'D2019.02.15_S012161_I0631_D_2'\n",
      "  'D2019.02.15_S012161_I0631_D_5'\n",
      "  'D2019.02.15_S012161_I0631_D_6'\n",
      "  'D2020.08.03_S02162_I0406_D_1'\n",
      "  'D2020.08.03_S02162_I0406_D_5'\n",
      "  'D2020.08.03_S02162_I0406_D_7'\n",
      "  'D2018.12.21_S02510_I0106_D_4'\n",
      "  'D2019.02.15_S012161_I0631_D_3'\n",
      "  'D2020.08.03_S02162_I0406_D_3'\n",
      "  'D2020.08.03_S02162_I0406_D_8'\n",
      "  'D2016.11.14_S1895_I106_8'\n",
      "  'D2016.11.14_S1895_I106_9'\n",
      "  'D2018.12.21_S02510_I0106_D_3'\n",
      "  'D2019.02.15_S012161_I0631_D_4'\n",
      "  'D2020.08.03_S02162_I0406_D_2'\n",
      "  'D2020.08.03_S02162_I0406_D_4'\n",
      "  'D2020.08.03_S02162_I0406_D_6'\n",
      "  'D2016.11.15_S0516_I631_1'\n",
      "  'D2016.11.15_S0516_I631_2'\n",
      "  'D2020.08.03_S00395_I3026_P_1'\n",
      "  'D2020.08.03_S00395_I3026_P_2'\n",
      "  'D2020.08.03_S00395_I3026_P_3'\n",
      "  'D2020.08.03_S00395_I3026_P_4'\n",
      "  'D2020.08.03_S00395_I3026_P_5'\n",
      "  'D2024.07.13_S01059_I4587_P_1'\n",
      "  'D2024.07.13_S01059_I4587_P_3'\n",
      "  'D2024.07.13_S01059_I4587_P_4'\n",
      "  'D2024.07.15_S01061_I4587_P_1'\n",
      "  'D2024.07.15_S01061_I4587_P_2'\n",
      "  'D2024.07.15_S01061_I4587_P_3'\n",
      "  'D2024.07.15_S01061_I4587_P_4'\n",
      "  'D2016.11.15_S1896_I106_6'\n",
      "  'D2020.08.04_S02163_I0406_D_3'\n",
      "  'D2016.11.15_S1896_I106_7'\n",
      "  'D2020.08.04_S02163_I0406_D_1'\n",
      "  'D2020.08.04_S02163_I0406_D_2'\n",
      "  'D2024.07.15_S01061_I4587_P_5'\n",
      "  'D2024.07.15_S01061_I4587_P_6'\n",
      "  'D2024.07.15_S01061_I4587_P_7'\n",
      "  'D2024.07.15_S01061_I4587_P_8'\n"
     ]
    }
   ],
   "source": [
    "# print all missing IDs from the previous analysis\n",
    "if 'df_merged' in locals():\n",
    "    missing_ids = df_merged[~df_merged['has_images']]['dish_well'].tolist()\n",
    "    print(\"\\n--- COMPLETE LIST OF MISSING IDs ---\")\n",
    "    for mid in missing_ids:\n",
    "        print(f\"  '{mid}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embryoValenciaVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
