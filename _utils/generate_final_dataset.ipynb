{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215bd492",
   "metadata": {},
   "source": [
    "## Improve original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b155cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Processing ---\n",
      "1. Processing DB_GV-rMII.xlsx (Sheet 1: GVs)...\n",
      "   -> Added 99 GVs.\n",
      "2. Processing DB_MII_to_blasto.xlsx (Sheet 1: Rescue MII)...\n",
      "   -> Added 51 Rescue MIIs.\n",
      "3. Processing DB_MII_to_blasto.xlsx (Sheet 2: Native MII)...\n",
      "   -> Added 60 Native MIIs.\n",
      "\n",
      "=== SUCCESS ===\n",
      "Total Rows Processed: 207\n",
      "Final Rows (after dedup): 206\n",
      "File saved to: /home/phd2/Scrivania/CorsoRepo/embryo_valencia/datasets/dataset_merged.csv\n",
      "\n",
      "Class Distribution:\n",
      "Note\n",
      "GV_to_M2            47\n",
      "rM2_to_no_blasto    42\n",
      "M2_to_blasto        39\n",
      "GV_to_GV            36\n",
      "M2_to_no_blasto     21\n",
      "GV_to_M1            12\n",
      "rM2_to_blasto        9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "# Paths\n",
    "DATASET_1_PATH = \"/home/phd2/Scrivania/CorsoRepo/embryo_valencia/datasets/DB_GV-rMII.xlsx\"\n",
    "DATASET_2_PATH = \"/home/phd2/Scrivania/CorsoRepo/embryo_valencia/datasets/DB_MII_to_blasto.xlsx\"\n",
    "OUTPUT_PATH    = \"/home/phd2/Scrivania/CorsoRepo/embryo_valencia/datasets/dataset_merged.csv\"\n",
    "\n",
    "# Target Columns in strict order\n",
    "FINAL_COLUMNS = [\n",
    "    \"dish_well\", \n",
    "    \"BLASTO_NY\", \n",
    "    \"GV\", \n",
    "    \"sibling\", \n",
    "    \"timing_GVBD\", \n",
    "    \"timing_extrusion_PB\", \n",
    "    \"Note\", \n",
    "    \"start video\", \n",
    "    \"start frame\", \n",
    "    \"end video\", \n",
    "    \"end frame\", \n",
    "    \"NOTES\", \n",
    "    \"NOTES 1\"\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def clean_pdb_col(val):\n",
    "    \"\"\"\n",
    "    Splits 'D2016...pdb GV-rMII' into ('D2016...', 'GV-rMII')\n",
    "    \"\"\"\n",
    "    if pd.isna(val): return \"\", \"\"\n",
    "    s = str(val).strip()\n",
    "    if \".pdb\" in s.lower():\n",
    "        idx = s.lower().find(\".pdb\") + 4\n",
    "        name = s[:idx].strip()\n",
    "        stage = s[idx:].strip()\n",
    "        clean_name = re.sub(r'\\.pdb$', '', name, flags=re.IGNORECASE)\n",
    "        return clean_name, stage\n",
    "    return s, \"\"\n",
    "\n",
    "def map_stage_to_code(stage_str):\n",
    "    \"\"\"Maps stage string (e.g. GV-rMII) to numeric GV code.\"\"\"\n",
    "    s = str(stage_str).upper().replace(\"-\", \"\").replace(\".\", \"\")\n",
    "    if \"RMII\" in s or \"MII\" in s: return 2\n",
    "    if \"MI\" in s: return 1\n",
    "    if \"GV\" in s: return 0\n",
    "    return np.nan\n",
    "\n",
    "def get_ivm_note(gv_code):\n",
    "    \"\"\"Maps numeric code to class label.\"\"\"\n",
    "    if gv_code == 0: return \"GV_to_GV\"\n",
    "    elif gv_code == 1: return \"GV_to_M1\"\n",
    "    elif gv_code == 2: return \"GV_to_M2\"\n",
    "    return \"Unknown_IVM\"\n",
    "\n",
    "def normalize_colnames(df):\n",
    "    \"\"\"\n",
    "    Robust column renaming. \n",
    "    Matches fuzzy names (e.g. 'starti video') to standard keys.\n",
    "    \"\"\"\n",
    "    col_map = {}\n",
    "    for c in df.columns:\n",
    "        cl = str(c).lower().strip()\n",
    "        \n",
    "        # Mapping rules\n",
    "        if \"notes 1\" in cl: col_map[c] = \"NOTES 1\"\n",
    "        elif \"notes\" in cl: col_map[c] = \"NOTES\"\n",
    "        elif \"start\" in cl and \"video\" in cl: col_map[c] = \"start video\"\n",
    "        elif \"start\" in cl and \"frame\" in cl: col_map[c] = \"start frame\"\n",
    "        elif \"end\" in cl and \"video\" in cl: col_map[c] = \"end video\"\n",
    "        elif \"end\" in cl and \"frame\" in cl: col_map[c] = \"end frame\"\n",
    "        elif \"gvbd\" in cl: col_map[c] = \"timing_GVBD\"\n",
    "        elif \"pb\" in cl and \"timing\" in cl: col_map[c] = \"timing_extrusion_PB\"\n",
    "        elif \"blasto\" in cl: col_map[c] = \"BLASTO_NY\"\n",
    "        # Preserve specific 'well' column, avoid matching 'dish_well'\n",
    "        elif \"well\" in cl and \"dish\" not in cl: col_map[c] = \"well\" \n",
    "        \n",
    "    return df.rename(columns=col_map)\n",
    "\n",
    "def clean_id_from_db2(val):\n",
    "    \"\"\"\n",
    "    Cleans IDs from DB2 which often look like 'D2016..._wells_1'.\n",
    "    Returns 'D2016..._1'.\n",
    "    \"\"\"\n",
    "    s = str(val).strip()\n",
    "    return re.sub(r'_wells_', '_', s, flags=re.IGNORECASE).strip()\n",
    "\n",
    "# ==========================================\n",
    "# 3. PROCESSING LOGIC\n",
    "# ==========================================\n",
    "\n",
    "dfs = []\n",
    "\n",
    "print(\"--- Starting Processing ---\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. DATASET 1: SHEET 1 (GV / IVM)\n",
    "# ---------------------------------------------------------\n",
    "print(\"1. Processing DB_GV-rMII.xlsx (Sheet 1: GVs)...\")\n",
    "try:\n",
    "    d1s1 = pd.read_excel(DATASET_1_PATH, sheet_name=0)\n",
    "    # Check if header is on row 0 or 1\n",
    "    if 'well' not in [str(c).lower() for c in d1s1.columns]:\n",
    "        d1s1 = pd.read_excel(DATASET_1_PATH, sheet_name=0, header=1)\n",
    "    \n",
    "    d1s1 = normalize_colnames(d1s1)\n",
    "    \n",
    "    # Find PDB column (contains .pdb)\n",
    "    pdb_col = next((c for c in d1s1.columns if d1s1[c].astype(str).str.contains(r'\\.pdb', case=False).any()), d1s1.columns[0])\n",
    "    \n",
    "    clean_rows = []\n",
    "    for idx, row in d1s1.iterrows():\n",
    "        name, stage = clean_pdb_col(row[pdb_col])\n",
    "        gv_val = map_stage_to_code(stage)\n",
    "        note = get_ivm_note(gv_val)\n",
    "        well = str(row.get('well', '')).split('.')[0].strip()\n",
    "        dish_well = f\"{name}_{well}\"\n",
    "        \n",
    "        new_row = {\n",
    "            \"dish_well\": dish_well,\n",
    "            \"BLASTO_NY\": pd.to_numeric(row.get('BLASTO_NY'), errors='coerce'),\n",
    "            \"GV\": gv_val,\n",
    "            \"sibling\": 0,\n",
    "            \"timing_GVBD\": row.get('timing_GVBD', np.nan),\n",
    "            \"timing_extrusion_PB\": row.get('timing_extrusion_PB', np.nan),\n",
    "            \"Note\": note,\n",
    "            \"start video\": row.get('start video', np.nan),\n",
    "            \"start frame\": row.get('start frame', np.nan),\n",
    "            \"end video\": row.get('end video', np.nan),\n",
    "            \"end frame\": row.get('end frame', np.nan),\n",
    "            \"NOTES\": row.get('NOTES', \"\"),\n",
    "            \"NOTES 1\": row.get('NOTES 1', \"\")\n",
    "        }\n",
    "        clean_rows.append(new_row)\n",
    "    \n",
    "    dfs.append(pd.DataFrame(clean_rows))\n",
    "    print(f\"   -> Added {len(clean_rows)} GVs.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Error D1S1: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. DATASET 2: SHEET 1 (Rescue MII)\n",
    "# ---------------------------------------------------------\n",
    "print(\"2. Processing DB_MII_to_blasto.xlsx (Sheet 1: Rescue MII)...\")\n",
    "try:\n",
    "    # Sheet 0 (first sheet) is usually Rescue MII (rM2)\n",
    "    d2s1 = pd.read_excel(DATASET_2_PATH, sheet_name=0)\n",
    "    d2s1 = normalize_colnames(d2s1)\n",
    "    \n",
    "    pdb_col = next((c for c in d2s1.columns if 'pdb' in str(c).lower()), d2s1.columns[0])\n",
    "    \n",
    "    clean_rows = []\n",
    "    for idx, row in d2s1.iterrows():\n",
    "        dish_well = clean_id_from_db2(row[pdb_col])\n",
    "        blasto = pd.to_numeric(row.get('BLASTO_NY'), errors='coerce')\n",
    "        # Logic: rM2\n",
    "        note = \"rM2_to_blasto\" if blasto == 1 else \"rM2_to_no_blasto\"\n",
    "        \n",
    "        new_row = {\n",
    "            \"dish_well\": dish_well,\n",
    "            \"BLASTO_NY\": blasto,\n",
    "            \"GV\": 3, # Rescue MII\n",
    "            \"sibling\": 0,\n",
    "            \"timing_GVBD\": np.nan,\n",
    "            \"timing_extrusion_PB\": np.nan,\n",
    "            \"Note\": note,\n",
    "            \"start video\": row.get('start video', np.nan),\n",
    "            \"start frame\": row.get('start frame', np.nan),\n",
    "            \"end video\": row.get('end video', np.nan),\n",
    "            \"end frame\": row.get('end frame', np.nan),\n",
    "            \"NOTES\": row.get('NOTES', \"\"),\n",
    "            \"NOTES 1\": row.get('NOTES 1', \"\")\n",
    "        }\n",
    "        clean_rows.append(new_row)\n",
    "        \n",
    "    dfs.append(pd.DataFrame(clean_rows))\n",
    "    print(f\"   -> Added {len(clean_rows)} Rescue MIIs.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Error D2S1: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. DATASET 2: SHEET 2 (Native MII)\n",
    "# ---------------------------------------------------------\n",
    "print(\"3. Processing DB_MII_to_blasto.xlsx (Sheet 2: Native MII)...\")\n",
    "try:\n",
    "    # Sheet 1 (second sheet) is Native MII (M2)\n",
    "    d2s2 = pd.read_excel(DATASET_2_PATH, sheet_name=1)\n",
    "    d2s2 = normalize_colnames(d2s2)\n",
    "    \n",
    "    pdb_col = next((c for c in d2s2.columns if 'pdb' in str(c).lower()), d2s2.columns[0])\n",
    "    \n",
    "    clean_rows = []\n",
    "    for idx, row in d2s2.iterrows():\n",
    "        dish_well = clean_id_from_db2(row[pdb_col])\n",
    "        blasto = pd.to_numeric(row.get('BLASTO_NY'), errors='coerce')\n",
    "        # Logic: Native M2\n",
    "        note = \"M2_to_blasto\" if blasto == 1 else \"M2_to_no_blasto\"\n",
    "        \n",
    "        new_row = {\n",
    "            \"dish_well\": dish_well,\n",
    "            \"BLASTO_NY\": blasto,\n",
    "            \"GV\": -1, # Native MII\n",
    "            \"sibling\": 1,\n",
    "            \"timing_GVBD\": np.nan,\n",
    "            \"timing_extrusion_PB\": np.nan,\n",
    "            \"Note\": note,\n",
    "            \"start video\": row.get('start video', np.nan),\n",
    "            \"start frame\": row.get('start frame', np.nan),\n",
    "            \"end video\": row.get('end video', np.nan),\n",
    "            \"end frame\": row.get('end frame', np.nan),\n",
    "            \"NOTES\": row.get('NOTES', \"\"),\n",
    "            \"NOTES 1\": row.get('NOTES 1', \"\")\n",
    "        }\n",
    "        clean_rows.append(new_row)\n",
    "        \n",
    "    dfs.append(pd.DataFrame(clean_rows))\n",
    "    print(f\"   -> Added {len(clean_rows)} Native MIIs.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Error D2S2: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. EXPORT\n",
    "# ==========================================\n",
    "if dfs:\n",
    "    df_total = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # 1. Filter out empty/short IDs\n",
    "    df_total = df_total[df_total['dish_well'].astype(str).str.len() > 5]\n",
    "    \n",
    "    # 2. Dedup (Keep first occurrence)\n",
    "    before_len = len(df_total)\n",
    "    df_total.drop_duplicates(subset='dish_well', keep='first', inplace=True)\n",
    "    \n",
    "    # 3. Ensure all columns exist and fill NaNs\n",
    "    for c in FINAL_COLUMNS:\n",
    "        if c not in df_total.columns:\n",
    "            df_total[c] = np.nan\n",
    "            \n",
    "    df_total = df_total[FINAL_COLUMNS]\n",
    "    \n",
    "    # 4. Save\n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "    df_total.to_csv(OUTPUT_PATH, index=False)\n",
    "    \n",
    "    print(\"\\n=== SUCCESS ===\")\n",
    "    print(f\"Total Rows Processed: {before_len}\")\n",
    "    print(f\"Final Rows (after dedup): {len(df_total)}\")\n",
    "    print(f\"File saved to: {OUTPUT_PATH}\")\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(df_total['Note'].value_counts())\n",
    "else:\n",
    "    print(\"\\nNo data processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717ec364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and create dataset_final_merged.csv from the dataset_merged selecting only the raws that have empty columns \"NOTES\" and \"NOTES 1\" \n",
    "df_merged = pd.read_csv(\"/home/phd2/Scrivania/CorsoRepo/embryo_valencia/datasets/dataset_merged.csv\")\n",
    "df_final = df_merged[\n",
    "    (df_merged[\"NOTES\"].isna() | (df_merged[\"NOTES\"].astype(str).str.strip() == \"\")) &\n",
    "    (df_merged[\"NOTES 1\"].isna() | (df_merged[\"NOTES 1\"].astype(str).str.strip() == \"\"))\n",
    "]\n",
    "df_final = df_final.drop(columns=[\"NOTES\", \"NOTES 1\"])\n",
    "df_final.to_csv(\"/home/phd2/Scrivania/CorsoRepo/embryo_valencia/datasets/dataset_final_merged2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98901431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview:\n",
      "                        dish_well  BLASTO_NY   GV  sibling timing_GVBD  \\\n",
      "10  D2019.02.15_S012161_I0631_D_1        0.0  2.0        0         0.6   \n",
      "11  D2019.02.15_S012161_I0631_D_2        0.0  2.0        0         5.6   \n",
      "12  D2019.02.15_S012161_I0631_D_5        1.0  2.0        0         4.6   \n",
      "13  D2019.02.15_S012161_I0631_D_6        0.0  2.0        0           -   \n",
      "14   D2019.03.14_S00126_I0758_D_3        0.0  2.0        0         7.1   \n",
      "15   D2019.03.14_S00126_I0758_D_6        0.0  2.0        0         9.6   \n",
      "16   D2019.03.14_S00126_I0758_D_7        0.0  2.0        0         9.2   \n",
      "17   D2019.05.26_S01883_I0406_D_9        0.0  2.0        0         1.4   \n",
      "18  D2019.05.26_S01883_I0406_D_10        0.0  2.0        0         2.4   \n",
      "19  D2019.05.26_S01883_I0406_D_11        0.0  2.0        0         0.9   \n",
      "\n",
      "   timing_extrusion_PB      Note start video  start frame  end video  \\\n",
      "10                13.8  GV_to_M2         0.1          1.0       24.3   \n",
      "11                20.8  GV_to_M2         0.1          1.0       24.3   \n",
      "12                20.6  GV_to_M2         0.1          1.0       24.3   \n",
      "13                17.3  GV_to_M2         0.1          1.0       24.3   \n",
      "14                21.4  GV_to_M2         2.1          1.0       23.9   \n",
      "15                21.1  GV_to_M2         2.1          1.0       23.9   \n",
      "16                14.7  GV_to_M2         2.1          1.0       23.9   \n",
      "17                14.4  GV_to_M2         0.2          1.0       23.3   \n",
      "18                16.2  GV_to_M2         0.2          1.0       23.3   \n",
      "19                17.7  GV_to_M2         0.2          1.0       23.3   \n",
      "\n",
      "    end frame NOTES NOTES 1  \n",
      "10       98.0   NaN     NaN  \n",
      "11       98.0   NaN     NaN  \n",
      "12       98.0   NaN     NaN  \n",
      "13       98.0   NaN     NaN  \n",
      "14       88.0   NaN     NaN  \n",
      "15       88.0   NaN     NaN  \n",
      "16       88.0   NaN     NaN  \n",
      "17       93.0   NaN     NaN  \n",
      "18       93.0   NaN     NaN  \n",
      "19       93.0   NaN     NaN  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPreview:\")\n",
    "print(df_total.iloc[10:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embryoValenciaVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
